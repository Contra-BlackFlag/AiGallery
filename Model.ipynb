{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717a0032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/contra/.local/lib/python3.13/site-packages (from faiss-cpu) (2.3.2)\n",
      "Requirement already satisfied: packaging in /home/contra/miniconda3/lib/python3.13/site-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.12.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b1c770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: torch in /home/contra/.local/lib/python3.13/site-packages (2.8.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.23.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: filelock in /home/contra/.local/lib/python3.13/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/contra/miniconda3/lib/python3.13/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/contra/.local/lib/python3.13/site-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/contra/miniconda3/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/contra/miniconda3/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.7.34-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/contra/miniconda3/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/contra/miniconda3/lib/python3.13/site-packages (from transformers) (0.21.4)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/contra/miniconda3/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/contra/.local/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/contra/miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/contra/miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
      "Requirement already satisfied: setuptools in /home/contra/miniconda3/lib/python3.13/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/contra/.local/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/contra/.local/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/contra/.local/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/contra/.local/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/contra/.local/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/contra/.local/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/contra/.local/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/contra/.local/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/contra/.local/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/contra/.local/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/contra/.local/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/contra/.local/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/contra/.local/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/contra/.local/lib/python3.13/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/contra/.local/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/contra/.local/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/contra/.local/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/contra/.local/lib/python3.13/site-packages (from torch) (3.4.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached pillow-11.3.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/contra/.local/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/contra/miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/contra/miniconda3/lib/python3.13/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/contra/miniconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/contra/.local/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n",
      "Downloading transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.23.0-cp313-cp313-manylinux_2_28_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pillow-11.3.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Downloading regex-2025.7.34-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m801.9/801.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Installing collected packages: safetensors, regex, pillow, MarkupSafe, transformers, torchvision\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6/6\u001b[0m [torchvision]\u001b[0m [torchvision]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 pillow-11.3.0 regex-2025.7.34 safetensors-0.6.2 torchvision-0.23.0 transformers-4.55.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e912201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Using the GPU.\n",
      "Model moved to cuda successfully!\n"
     ]
    }
   ],
   "source": [
    "# What you're about to do:\n",
    "# We'll import torch, check for a GPU, and then move our loaded model\n",
    "# to the selected device (GPU or CPU) for faster processing.\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# 1. Check for GPU availability and set the device\n",
    "# torch.cuda.is_available() returns True if a CUDA-enabled GPU is found.\n",
    "if torch.cuda.is_available():\n",
    "    # Set the device to the first available GPU ('cuda:0')\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Using the GPU.\")\n",
    "else:\n",
    "    # If no GPU is found, set the device to CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU found. Using the CPU.\")\n",
    "\n",
    "# 2. Load the model and processor as before\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 3. Move the model to the selected device (GPU or CPU)\n",
    "# The .to() method moves the model's parameters and buffers to the specified device.\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model moved to {device} successfully!\")\n",
    "\n",
    "# The 'model' is now ready for fast computations on the GPU (if available).\n",
    "# The 'processor' does not need to be moved as it primarily works on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0fdc622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in /home/contra/miniconda3/lib/python3.13/site-packages (11.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4259297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded successfully.\n",
      "\n",
      "Embedding generated successfully!\n",
      "Shape of the embedding: torch.Size([1, 512])\n",
      "Here are the first few values of the embedding:\n",
      "tensor([ 0.1851,  0.2269,  0.0139,  0.1115, -0.3792,  0.1189, -0.3501,  0.1943,\n",
      "         0.9519,  0.1973], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# What you're about to do:\n",
    "# We will load a single image, use the processor to prepare it,\n",
    "# and then use the model to extract its feature embedding.\n",
    "\n",
    "import torch\n",
    "from PIL import Image # The Pillow library for image handling\n",
    "\n",
    "# Make sure you have the 'device', 'model', and 'processor' variables\n",
    "# from the previous script loaded in your environment.\n",
    "\n",
    "# --- Step 1: Open an Image ---\n",
    "# IMPORTANT: Replace this with the actual path to one of your images.\n",
    "image_path = \"MewithCAt.jpg\"\n",
    "\n",
    "try:\n",
    "    # This opens the image file.\n",
    "    raw_image = Image.open(image_path)\n",
    "    print(\"Image loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{image_path}' was not found.\")\n",
    "    # Exit or handle the error appropriately\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Step 2 & 3: Preprocess the Image and Move to GPU ---\n",
    "# The processor prepares the image and returns a dictionary.\n",
    "# We specify 'return_tensors=\"pt\"' to get PyTorch tensors.\n",
    "# The .to(device) moves the processed tensor to the GPU.\n",
    "inputs = processor(images=raw_image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "# --- Step 4: Get the Embedding ---\n",
    "# We use torch.no_grad() because we are not training the model,\n",
    "# which saves memory and computation.\n",
    "with torch.no_grad():\n",
    "    # This function call passes the image through the model\n",
    "    # and gets the final embedding vector.\n",
    "    image_features = model.get_image_features(**inputs)\n",
    "\n",
    "# The output is our embedding!\n",
    "print(\"\\nEmbedding generated successfully!\")\n",
    "print(\"Shape of the embedding:\", image_features.shape)\n",
    "print(\"Here are the first few values of the embedding:\")\n",
    "print(image_features[0, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d04aa9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text: 'rocket'\n",
      "\n",
      "Text embedding generated successfully!\n",
      "Shape of the embedding: torch.Size([1, 512])\n",
      "Here are the first few values of the embedding:\n",
      "tensor([ 0.1666, -0.0631,  0.1757,  0.2560,  0.0776,  0.2820,  0.0721, -1.2316,\n",
      "        -0.2682,  0.4632], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# What you're about to do:\n",
    "# We will take a sample text description, process it using the tokenizer,\n",
    "# and then use the model to extract its feature embedding.\n",
    "\n",
    "import torch\n",
    "\n",
    "# Make sure you have the 'device', 'model', and 'processor' variables\n",
    "# from the previous scripts.\n",
    "\n",
    "# --- Step 1: Define Text ---\n",
    "# Let's create a sample text description. You can change this to anything!\n",
    "text_description = \"rocket\"\n",
    "print(f\"Processing text: '{text_description}'\")\n",
    "\n",
    "\n",
    "# --- Step 2 & 3: Preprocess the Text and Move to GPU ---\n",
    "# The processor tokenizes the text and returns PyTorch tensors.\n",
    "# 'padding=True' ensures all sentences are the same length (important for batches).\n",
    "# 'truncation=True' cuts off text that is too long for the model.\n",
    "inputs = processor(text=text_description, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "\n",
    "# --- Step 4: Get the Embedding ---\n",
    "# Again, we use torch.no_grad() for efficiency.\n",
    "with torch.no_grad():\n",
    "    # This function call passes the text through the model\n",
    "    # and gets the final embedding vector.\n",
    "    text_features = model.get_text_features(**inputs)\n",
    "\n",
    "# The output is our text embedding!\n",
    "print(\"\\nText embedding generated successfully!\")\n",
    "print(\"Shape of the embedding:\", text_features.shape)\n",
    "print(\"Here are the first few values of the embedding:\")\n",
    "print(text_features[0, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e17ac92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Score: 0.2193\n",
      "This might be a potential match. ğŸ¤”\n"
     ]
    }
   ],
   "source": [
    "# What you're about to do:\n",
    "# We will calculate the cosine similarity between the image and text embeddings\n",
    "# to get a final score of how well they match.\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Make sure you have the 'image_features' and 'text_features' tensors\n",
    "# from the previous steps.\n",
    "\n",
    "# --- Step 1: Normalize the embeddings ---\n",
    "# Normalizing the vectors is a standard step before comparison.\n",
    "image_features_norm = F.normalize(image_features, p=2, dim=-1)\n",
    "text_features_norm = F.normalize(text_features, p=2, dim=-1)\n",
    "\n",
    "\n",
    "# --- Step 2: Calculate the dot product (cosine similarity) ---\n",
    "# We use matrix multiplication (@) to compute the dot product.\n",
    "similarity_score = torch.matmul(image_features_norm, text_features_norm.T)\n",
    "\n",
    "\n",
    "# --- Step 3: Extract and print the score ---\n",
    "# .item() extracts the single value from the tensor.\n",
    "final_score = similarity_score.item()\n",
    "\n",
    "print(f\"\\nSimilarity Score: {final_score:.4f}\")\n",
    "\n",
    "# Let's add some context to the score\n",
    "if final_score > 0.25:\n",
    "    print(\"This looks like a good match! âœ…\")\n",
    "elif final_score > 0.18:\n",
    "    print(\"This might be a potential match. ğŸ¤”\")\n",
    "else:\n",
    "    print(\"This is likely not a good match. âŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7041487e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce2b0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
